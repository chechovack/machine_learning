{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPCeoTW7rGd8mJ9Oog+A3cG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Clasificadores\n","\n","### Introducción:\n","\n","\n","\n","En esta clase, nos enfocaremos en algunos de los algoritmos más poderosos y populares para la clasificación, como **Support Vector Machines (SVM)**, **Árboles de Decisión**, **Random Forest**, y los algoritmos avanzados de boosting como **XGBoost** y **LightGBM**. Cada uno de estos métodos tiene sus propias fortalezas, debilidades y áreas de aplicación, por lo que es crucial entender no solo cómo funcionan, sino también cuándo y por qué elegir uno sobre otro.\n","\n","A lo largo de la clase, exploraremos:\n","1. **SVM**, que es particularmente útil cuando trabajamos con datos de alta dimensionalidad y necesitamos encontrar la mejor frontera de decisión.\n","2. **Árboles de Decisión**, que nos permiten tomar decisiones claras y visualmente intuitivas, pero que pueden ser propensos al sobreajuste.\n","3. **Random Forest**, que mejora la estabilidad y precisión de los árboles de decisión al combinar múltiples árboles para generar predicciones robustas.\n","4. **XGBoost** y **LightGBM**, que utilizan técnicas avanzadas de boosting para corregir iterativamente los errores de los modelos previos y lograr resultados altamente precisos en problemas complejos.\n","\n","Al final de esta clase, tendrán una comprensión clara de cómo usar estos métodos en sus propios proyectos de machine learning y cómo interpretar los resultados obtenidos.\n","\n","# SVM\n","\n","El **SVM** busca encontrar un hiperplano que maximice el margen entre las clases en un espacio de características, donde solo los vectores de soporte (los puntos más cercanos al hiperplano) definen su posición y orientación.\n","\n","Para trabajar con datos no linealmente separables, **SVM** usa funciones de kernel como el lineal, polinómico o RBF, lo que permite operar en espacios de características más altos sin calcular explícitamente sus coordenadas. El ajuste de los hiperparámetros como **C** (que controla el margen y los errores de clasificación) y **Gamma** (que influye en la complejidad de los límites de decisión) es clave para optimizar el modelo.\n","\n","Las ventajas de **SVM** incluyen su eficacia en espacios de alta dimensión y su capacidad de manejar datos no lineales mediante el uso de kernels. Sin embargo, puede ser ineficiente para conjuntos de datos muy grandes y requiere ajustes cuidadosos del kernel y los parámetros.\n","\n","\n"],"metadata":{"id":"WCk1jNjJSR9w"}},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn import datasets\n","import pandas as pd\n","import numpy as np\n","import warnings\n","\n","# Cargar el conjunto de datos iris\n","iris = datasets.load_iris()\n","\n","\n","\n","# Extraer las características (columnas) del dataset Iris\n","X = pd.DataFrame(iris[\"data\"], columns=iris[\"feature_names\"])\n","\n","\n","# Crear el vector objetivo (1 si es Iris-Virginica, de lo contrario 0)\n","y = iris[\"target\"]   # 0 para Iris-Setosa, 1 para Iris-Versicolor, 2 para Iris-Virginica\n","\n","\n","# Dividir los datos en conjuntos de entrenamiento y prueba\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1, stratify=y)\n","\n","# Normalizar los datos: utilizar la media y desviación estándar del conjunto de entrenamiento\n","mean_X_train = np.mean(X_train, axis=0)\n","std_X_train = np.std(X_train, axis=0)\n","\n","X_train_z = (X_train - mean_X_train) / std_X_train  # Normalización de entrenamiento\n","X_test_z = (X_test - mean_X_train) / std_X_train    # Normalización de prueba con estadísticas de entrenamiento\n","\n","X_train_z=X_train_z[['petal length (cm)','petal width (cm)']]\n","X_test_z=X_test_z[['petal length (cm)','petal width (cm)']]\n","\n","\n","\n"],"metadata":{"id":"trxaaWUyUM5y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Un hiperparametro importante para el modelo SVM es el  **hiperparámetro $C$** el cual controla el equilibrio entre la maximización del margen y la minimización de los errores de clasificación. Es esencial en los **Soft Margin SVM**, donde los datos pueden no ser separables linealmente, permitiendo que algunos puntos caigan dentro del margen o se clasifiquen incorrectamente.\n","\n","### Función de $C$:\n","El parámetro $C$ establece la importancia que le damos a los **errores de clasificación** en comparación con el **tamaño del margen**. Se ajusta en la función objetivo del modelo, la cual intenta minimizar tanto el tamaño del margen como los errores cometidos por el modelo. Matemáticamente, la función objetivo es:\n","\n","$$\n","\\frac{1}{2} ||w||^2 + C \\sum_{i=1}^{n} \\xi_i\n","$$\n","\n","Donde:\n","- $||w||^2$ está inversamente relacionado con el margen en **SVM**: un margen pequeño implica un valor grande de $||w||^2$, y un margen grande implica un valor pequeño. Minimizar $||w||^2$ maximiza el margen.\n","- $\\xi_i$ son las **variables de holgura** que permiten errores dentro del margen.\n","- $C$ es un hiperparámetro que equilibra un margen grande con la cantidad de errores permitidos.\n","\n","### Efecto del parámetro $C$:\n","1. **$C$ grande**:\n","   - El modelo prioriza **minimizar los errores** de clasificación.\n","   - El margen será más estrecho, permitiendo menos puntos dentro del margen o mal clasificados.\n","   - En este caso, el modelo se vuelve más **estricto** y tiende a ser **menos tolerante a errores**.\n","   - Sin embargo, al ser más estricto, también puede sobreajustar los datos (overfitting), lo que significa que el modelo puede ajustarse demasiado a los datos de entrenamiento, pero podría no generalizar bien a los datos de prueba.\n","\n","2. **$C$ pequeño**:\n","   - El modelo prioriza **maximizar el margen** sobre la reducción de errores.\n","   - Permite más errores de clasificación, ya que los puntos que están mal clasificados o dentro del margen reciben menos penalización.\n","   - Un $C$ más pequeño hace que el modelo sea **más flexible** y puede evitar el sobreajuste, resultando en un mejor ajuste a los datos de prueba (mejor generalización).\n","   - Sin embargo, un $C$ muy pequeño puede hacer que el modelo subajuste (underfitting) y no capture correctamente las relaciones en los datos de entrenamiento.\n","\n","### Visualización:\n","- **$C$ grande**: El modelo intentará ajustar casi todos los puntos de datos correctamente, incluso si eso significa que el margen será más pequeño.\n","\n","\n","- **$C$ pequeño**: El modelo tolerará más errores y buscará un margen más amplio, aunque algunos puntos queden dentro del margen o mal clasificados.\n","  \n","\n","\n"],"metadata":{"id":"v-iEUz0YVmM_"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn import svm\n","from sklearn.datasets import make_classification\n","\n","# Crear un conjunto de datos simple para clasificación binaria\n","X, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, random_state=42)\n","\n","# Crear dos clasificadores SVM con diferentes valores de C\n","clf_high_C = svm.SVC(kernel='linear', C=100)\n","clf_low_C = svm.SVC(kernel='linear', C=0.01)\n","\n","# Ajustar los modelos\n","clf_high_C.fit(X, y)\n","clf_low_C.fit(X, y)\n","\n","# Crear un grid para las visualizaciones\n","xx, yy = np.meshgrid(np.linspace(X[:, 0].min()-1, X[:, 0].max()+1, 100),\n","                     np.linspace(X[:, 1].min()-1, X[:, 1].max()+1, 100))\n","\n","# Predicción en el grid para obtener las fronteras de decisión\n","Z_high_C = clf_high_C.decision_function(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n","Z_low_C = clf_low_C.decision_function(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n","\n","# Visualización de los modelos\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n","\n","# Gráfico para C grande (ajuste más estricto)\n","ax1.contourf(xx, yy, Z_high_C, levels=np.linspace(Z_high_C.min(), 0, 7), cmap='coolwarm', alpha=0.5)\n","ax1.contour(xx, yy, Z_high_C, levels=[0], linewidths=2, colors='black')\n","ax1.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap='coolwarm', edgecolors='k')\n","ax1.set_title('SVM con C alto (C=100)')\n","\n","# Gráfico para C pequeño (margen más amplio)\n","ax2.contourf(xx, yy, Z_low_C, levels=np.linspace(Z_low_C.min(), 0, 7), cmap='coolwarm', alpha=0.5)\n","ax2.contour(xx, yy, Z_low_C, levels=[0], linewidths=2, colors='black')\n","ax2.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap='coolwarm', edgecolors='k')\n","ax2.set_title('SVM con C bajo (C=0.01)')\n","\n","plt.show()\n"],"metadata":{"id":"hwIACFjdW9gX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ahora teniendo los conceptos básicos de SVM, se va aplicar a nuestro problema del conjunto iris"],"metadata":{"id":"IgMgWcO9YoWR"}},{"cell_type":"code","source":["from sklearn.svm import SVC\n","svm=SVC(kernel='linear',C=1,random_state=1)\n","svm.fit(X_train_z,y_train)\n","\n"],"metadata":{"id":"f7PIphCtYz2z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np\n","from matplotlib.colors import ListedColormap\n","\n","# Función para graficar las regiones de decisión\n","def plot_decision_region(clf, X, y, test_idx=None, resolution=0.02):\n","    # Configurar los colores del mapa y los puntos\n","    markers = ('s', 'x', 'o', '^', 'v')\n","    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n","    cmap = ListedColormap(colors[:len(np.unique(y))])\n","\n","    # Seleccionar dos características para visualizar\n","    X = X.iloc[:, :2].values  # Solo las dos primeras columnas para graficar en 2D\n","\n","    # Crear los límites del gráfico\n","    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n","    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n","    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n","                           np.arange(x2_min, x2_max, resolution))\n","\n","    # Obtener las predicciones para la cuadrícula\n","    Z = clf.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n","    Z = Z.reshape(xx1.shape)\n","\n","    # Graficar la superficie de decisión\n","    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n","    plt.xlim(xx1.min(), xx1.max())\n","    plt.ylim(xx2.min(), xx2.max())\n","\n","    # Graficar los puntos de entrenamiento\n","    for idx, cl in enumerate(np.unique(y)):\n","        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],\n","                    alpha=0.8, c=colors[idx],\n","                    marker=markers[idx], label=f'Clase {cl}', edgecolor='black')\n","\n","    plt.xlabel('Primera característica')\n","    plt.ylabel('Segunda característica')\n","    plt.legend(loc='upper left')\n","    plt.show()\n","\n","# Entrenar un SVM con solo dos características para visualizar\n","from sklearn.svm import SVC\n","\n","# Utilizar solo las primeras dos características para la visualización\n","X_train_2D = X_train_z.iloc[:, :2]  # Seleccionamos solo las primeras 2 columnas\n","y_train_2D = y_train\n","\n","# Definir el clasificador SVM\n","svm_clf = SVC(kernel='linear', C=1.0, random_state=1)\n","svm_clf.fit(X_train_2D, y_train_2D)\n","\n","# Graficar las regiones de decisión\n","plot_decision_region(svm_clf, X_train_2D, y_train_2D)\n"],"metadata":{"id":"Iqc9UPOycOXE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Solución de problemas no lineales\n","\n","Con **SVM** también es posible resolver problemas no lineales mediante el uso de **kernels**. Al cambiar el kernel, podemos seleccionar entre diferentes tipos como **linear**, **poly** (polinómico), **rbf** (radial), y **sigmoid**. A continuación, veremos un ejemplo sintético que demuestra cómo funciona este proceso."],"metadata":{"id":"2JtnhBHNqZhV"}},{"cell_type":"code","source":["from sklearn.datasets import make_circles\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.svm import SVC\n","import matplotlib.pyplot as plt\n","import numpy as np\n","# Crear un conjunto de datos sintético\n","X, y = make_circles(n_samples=300, factor=0.5, noise=0.1, random_state=42)\n","\n","# Visualizar los datos\n","plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm')\n","plt.title('Datos Simulados')\n","plt.xlabel('Característica 1')\n","plt.ylabel('Característica 2')\n","plt.show()\n"],"metadata":{"id":"w3LaGM11a_ak"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Escalar los datos\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# Definir los modelos con diferentes kernels\n","kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n","models = [SVC(kernel=kernel, gamma='scale',C=2).fit(X_scaled, y) for kernel in kernels]\n","\n","# Crear una malla para visualizar los límites de decisión\n","xx, yy = np.meshgrid(np.linspace(X_scaled[:, 0].min() - 1, X_scaled[:, 0].max() + 1, 500),\n","                     np.linspace(X_scaled[:, 1].min() - 1, X_scaled[:, 1].max() + 1, 500))\n","grid = np.c_[xx.ravel(), yy.ravel()]\n","\n","# Visualizar los límites de decisión\n","fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n","titles = ['Kernel Lineal', 'Kernel Polinómico', 'Kernel RBF', 'Kernel Sigmoide']\n","\n","for model, kernel, ax, title in zip(models, kernels, axes.ravel(), titles):\n","    Z = model.decision_function(grid).reshape(xx.shape)\n","    ax.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7), cmap='coolwarm', alpha=0.5)\n","    ax.contour(xx, yy, Z, levels=[0], linewidths=2, colors='black')\n","    ax.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y, cmap='coolwarm', edgecolors='k')\n","    ax.set_title(title)\n","    ax.set_xlabel('Característica 1')\n","    ax.set_ylabel('Característica 2')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"ODsUYdxmrMOI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Árboles de decisiones\n","\n","El **Árbol de Decisión** es un modelo de clasificación que crea una estructura jerárquica de decisiones, donde cada nodo interno representa una característica o atributo, y cada hoja representa una clase o etiqueta. El objetivo es dividir el espacio de características en regiones puras que maximicen la separación entre las clases. Los criterios comunes para elegir las divisiones son el **índice de Gini** o la **entropía**, que miden la impureza de las particiones. El proceso de división continúa hasta que las hojas contienen muestras homogéneas o hasta que se alcanza una profundidad máxima.\n","\n","Las ventajas de los **Árboles de Decisión** incluyen su facilidad de interpretación y la capacidad de manejar tanto datos numéricos como categóricos. Sin embargo, tienden a sobreajustarse en los datos de entrenamiento si no se controla su profundidad. Para mitigar esto, es crucial ajustar hiperparámetros como la **profundidad máxima** o el **número mínimo de muestras por hoja**. Aunque son eficientes en conjuntos de datos pequeños a medianos, pueden volverse menos precisos en problemas más complejos o cuando los datos son ruidosos.\n"],"metadata":{"id":"h_85pqtbs-t1"}},{"cell_type":"markdown","source":["\n","### Creación de un árbol de decisiones\n","\n","Los árboles de decisión pueden construir **fronteras de decisión complejas** al dividir el espacio de características en regiones rectangulares o cúbicas (en espacios de mayor dimensión). Sin embargo, es importante tener cuidado, ya que **cuanto más profundo sea el árbol**, más específicas y detalladas serán las divisiones, lo que puede hacer que el modelo se ajuste demasiado a los datos de entrenamiento (sobreajuste), perdiendo capacidad de generalización.\n"],"metadata":{"id":"vL9rP1DVtT59"}},{"cell_type":"code","source":["from sklearn.tree import DecisionTreeClassifier\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Crear y ajustar el modelo del árbol de decisión\n","tree_model = DecisionTreeClassifier(criterion='gini', max_depth=4, random_state=1)\n","tree_model.fit(X_train_z.iloc[:, :2], y_train)  # Usamos solo las dos primeras características\n","\n","# Definir la función de visualización mejorada de las regiones de decisión\n","def plot_decision_region(clf, X, y, resolution=0.02):\n","    # Crear una paleta de colores y marcar diferentes clases\n","    markers = ('o', 's', '^', 'v')\n","    colors = ('#FF9999', '#66B2FF', '#99FF99')\n","    cmap = plt.cm.RdYlBu\n","\n","    # Seleccionar solo las primeras dos características para la visualización\n","    X = X.iloc[:, :2].values  # Convertir a numpy array para asegurar que sea manejable\n","\n","    # Crear los límites de los ejes\n","    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n","    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n","    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n","                           np.arange(x2_min, x2_max, resolution))\n","\n","    # Predecir las etiquetas para cada punto de la malla\n","    Z = clf.predict(np.c_[xx1.ravel(), xx2.ravel()])\n","    Z = Z.reshape(xx1.shape)\n","\n","    # Crear la superficie de decisión\n","    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)\n","\n","    # Ajustar límites de los ejes\n","    plt.xlim(xx1.min(), xx1.max())\n","    plt.ylim(xx2.min(), xx2.max())\n","\n","    # Graficar los puntos de datos\n","    for idx, cl in enumerate(np.unique(y)):\n","        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1], alpha=0.8, color=colors[idx],\n","                    edgecolor='k', marker=markers[idx], label=f'Clase {cl}', s=100)\n","\n","# Llamar a la función mejorada para graficar\n","plot_decision_region(tree_model, X_train_z.iloc[:, :2], y_train)\n","\n","# Configurar el título y las etiquetas del gráfico\n","plt.title('Árbol de Decisión - Regiones de Decisión')\n","plt.xlabel('Característica 1 (Estandarizada)')\n","plt.ylabel('Característica 2 (Estandarizada)')\n","plt.legend(loc='upper left')\n","plt.grid(True)\n","plt.show()\n"],"metadata":{"id":"jj2xYVVMu5o2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Una característica importante de los **árboles de decisión** es que nos permiten visualizar el proceso de toma de decisiones en cada nodo. Cada árbol puede ser analizado individualmente para entender cómo se dividen los datos y cómo se llegan a las predicciones, lo que facilita la interpretación de los patrones aprendidos por el modelo."],"metadata":{"id":"Um9LvSxjwQGy"}},{"cell_type":"code","source":["from sklearn import tree\n","import matplotlib.pyplot as plt\n","\n","# Definir los nombres de las características y las clases\n","features_names = ['sepal length', 'sepal width', 'petal length', 'petal width']\n","target_names = ['setosa', 'versicolor', 'virginica']\n","\n","# Graficar el primer árbol de decisión del modelo Random Forest\n","plt.figure(figsize=(12, 8))  # Ajustar el tamaño de la figura si es necesario\n","tree.plot_tree(tree_model,\n","               feature_names=features_names,\n","               class_names=target_names,\n","               filled=True,\n","               fontsize=10)\n","plt.show()\n","\n"],"metadata":{"id":"A-6cDtcxv_Io"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Combinacion de multiples arboles de decisión mediante bosques aleatorios\n","\n"],"metadata":{"id":"6VEf-HW4yTrx"}},{"cell_type":"markdown","source":["**Random Forest** es un algoritmo de aprendizaje supervisado que combina varios **árboles de decisión** independientes para mejorar la precisión de las predicciones y reducir el riesgo de sobreajuste (**overfitting**).\n","\n","### Proceso básico:\n","1. **Generación de submuestras**: Se crean múltiples árboles de decisión, cada uno entrenado con una submuestra diferente del conjunto de datos original, obtenida mediante **muestreo aleatorio con reemplazo** (bagging).\n","   \n","2. **Selección aleatoria de características**: En cada nodo de cada árbol, se selecciona aleatoriamente un subconjunto de características para decidir la mejor división. Esto introduce variabilidad entre los árboles y reduce la correlación entre ellos.\n","\n","3. **Promedio o voto**: En problemas de regresión, las predicciones de los árboles se **promedian**. En problemas de clasificación, los árboles votan por la clase más frecuente, y el resultado final es el que obtiene más votos (**votación por mayoría**).\n","\n","### Ventajas:\n","- **Robustez**: Al promediar los resultados de muchos árboles, se reduce el riesgo de sobreajuste.\n","- **Manejo de características**: Es capaz de manejar automáticamente características numéricas y categóricas, y no requiere normalización o estandarización.\n","\n","En resumen, **Random Forest** es un algoritmo poderoso que utiliza múltiples árboles de decisión para generar predicciones más precisas y generalizables."],"metadata":{"id":"NdHgi7QGy-kA"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n","# Crear y entrenar el modelo de Random Forest\n","# 'n_estimators=25' indica que se construirán 25 árboles de decisión.\n","# 'n_jobs=2' permite utilizar 2 núcleos de CPU para el entrenamiento en paralelo, lo que mejora la velocidad.\n","rf_model = RandomForestClassifier(n_estimators=25, random_state=1, n_jobs=2)\n","\n","# Ajustar el modelo con el conjunto de entrenamiento.\n","# X_train_z son las características estandarizadas del conjunto de entrenamiento, y y_train son las etiquetas.\n","rf_model.fit(X_train_z, y_train)\n","\n","plot_decision_region(rf_model, X_train_z, y_train)\n","\n","# Configurar el título y las etiquetas del gráfico\n","plt.title('Random Forest - Regiones de Decisión')  # Título del gráfico\n","plt.xlabel('Característica 1 (Estandarizada)')  # Etiqueta del eje X\n","plt.ylabel('Característica 2 (Estandarizada)')  # Etiqueta del eje Y\n","plt.legend(loc='upper left')  # Colocar la leyenda en la parte superior izquierda\n","plt.grid(True)  # Mostrar la cuadrícula para una mejor visualización\n","plt.show()  # Mostrar la gráfica en pantalla\n"],"metadata":{"id":"i5O9zBtdyB2N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Gradient Boosting\n","\n","Es un método de aprendizaje supervisado que combina múltiples modelos débiles, generalmente árboles de decisión, para crear un modelo fuerte. La idea principal detrás del **boosting** es entrenar modelos de manera secuencial, donde cada modelo nuevo trata de corregir los errores cometidos por los modelos anteriores. En el caso de **Gradient Boosting**, esto se hace utilizando el **gradiente** de la función de pérdida para ajustar cada nuevo modelo.\n","\n","### Funcionamiento de **Gradient Boosting**:\n","1. **Inicialización del modelo**:\n","   - El proceso comienza creando un modelo inicial, que puede ser un modelo muy simple, como un árbol de decisión con una sola división.\n","   - Este primer modelo genera predicciones iniciales.\n","\n","2. **Calcular los residuos**:\n","   - En cada iteración, se calculan los **residuos** o **errores** (diferencia entre las predicciones actuales y los valores verdaderos).\n","   - Estos residuos indican qué tan lejos están las predicciones del modelo actual de los valores reales.\n","\n","3. **Entrenamiento de un nuevo modelo**:\n","   - Un nuevo modelo se entrena para predecir los residuos de las iteraciones anteriores.\n","   - En lugar de predecir los valores directamente, el nuevo modelo predice las **correcciones** que deben aplicarse a las predicciones anteriores.\n","\n","4. **Actualización de las predicciones**:\n","   - Las predicciones se actualizan sumando las correcciones predichas por el nuevo modelo.\n","   - Este proceso se repite, y cada nuevo modelo intenta mejorar las predicciones anteriores ajustándose a los errores que se cometieron.\n","\n","5. **Tasa de aprendizaje (learning rate)**:\n","   - El ajuste de las predicciones no se hace de manera abrupta. Para controlar la contribución de cada modelo, se introduce un **learning rate** (tasa de aprendizaje) que reduce el impacto de cada nuevo modelo en la predicción final. Esto ayuda a evitar el sobreajuste.\n","\n","### Ventajas de Gradient Boosting:\n","1. **Alta precisión**: Es uno de los algoritmos más precisos en muchos problemas de clasificación y regresión.\n","2. **Capacidad de manejo de datos no lineales**: Los árboles de decisión pueden capturar relaciones no lineales en los datos, y el boosting mejora estas capacidades.\n","3. **Flexibilidad**: Se puede aplicar a una variedad de funciones de pérdida (error cuadrático, log-loss, etc.), lo que lo hace adecuado tanto para problemas de clasificación como de regresión.\n","\n","### Desventajas de Gradient Boosting:\n","1. **Riesgo de sobreajuste**: Debido a que el modelo se ajusta a los residuos en cada paso, puede sobreajustarse si no se controla adecuadamente (por ejemplo, si el número de árboles es muy alto o la profundidad de los árboles es grande).\n","2. **Coste computacional**: Dado que los modelos se entrenan secuencialmente, puede ser más lento en comparación con otros algoritmos como **Random Forest**, donde los árboles se entrenan en paralelo.\n","\n"],"metadata":{"id":"U_soqQtg1x-O"}},{"cell_type":"code","source":["from sklearn.ensemble import GradientBoostingClassifier\n","\n","# Crear el modelo Gradient Boosting\n","gb_model = GradientBoostingClassifier(n_estimators=30, learning_rate=0.1, max_depth=3, random_state=1)\n","\n","# Entrenar el modelo con los datos de entrenamiento estandarizados\n","gb_model.fit(X_train_z, y_train)\n","\n","# Hacer predicciones en el conjunto de prueba estandarizado\n","y_pred = gb_model.predict(X_test_z)\n","\n","# Evaluar el rendimiento del modelo\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f'Precisión del modelo Gradient Boosting: {accuracy:.4f}')\n"],"metadata":{"id":"Pc7hpS3614eE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**XGBoost** y **LightGBM** son dos poderosos algoritmos de **boosting**, una técnica de ensemble que combina varios modelos débiles (generalmente árboles de decisión) para crear un modelo fuerte que mejora el rendimiento de las predicciones. Ambos son muy eficientes en términos de velocidad y rendimiento, lo que los hace populares en competiciones y problemas de machine learning de gran escala.\n","\n","A continuación, te explico cómo funcionan y cómo aplicarlos en un conjunto de datos.\n","\n","---\n","\n","### **XGBoost (Extreme Gradient Boosting)**\n","\n","**XGBoost** es una implementación optimizada del algoritmo de **Gradient Boosting**. Este algoritmo construye árboles de decisión en secuencia, donde cada nuevo árbol intenta corregir los errores cometidos por los anteriores, enfocándose en las muestras mal clasificadas.\n","\n","#### **Pasos para aplicar XGBoost**:\n","1. **Cargar y preparar los datos**.\n","2. **Dividir los datos en conjuntos de entrenamiento y prueba**.\n","3. **Entrenar el modelo de XGBoost** utilizando las características del conjunto de entrenamiento.\n","4. **Evaluar el modelo** en el conjunto de prueba.\n","\n","\n"],"metadata":{"id":"9F6_VHN53DQ6"}},{"cell_type":"code","source":["\n","import xgboost as xgb\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","# Crear el modelo XGBoost\n","xgb_model = xgb.XGBClassifier(n_estimators=30, learning_rate=0.1, max_depth=3, random_state=1)\n","\n","# Entrenar el modelo con los datos de entrenamiento\n","xgb_model.fit(X_train_z, y_train)\n","\n","# Hacer predicciones en el conjunto de prueba\n","y_pred = xgb_model.predict(X_test_z)\n","\n","# Evaluar el rendimiento del modelo\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f'Precisión del modelo XGBoost: {accuracy:.4f}')\n"],"metadata":{"id":"2hl9zqz447Nd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np\n","from matplotlib.colors import ListedColormap\n","\n","# Definir la función de visualización para las regiones de decisión\n","def plot_decision_region(clf, X, y, resolution=0.02):\n","    # Crear una paleta de colores y marcar diferentes clases\n","    markers = ('o', 's', '^', 'v')\n","    colors = ('#FF9999', '#66B2FF', '#99FF99')\n","    cmap = ListedColormap(colors[:len(np.unique(y))])\n","\n","    # Seleccionar solo las primeras dos características para la visualización\n","    X = X.iloc[:, :2].values  # Convertir a numpy array con las dos primeras características\n","\n","    # Crear los límites de los ejes\n","    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n","    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n","    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n","                           np.arange(x2_min, x2_max, resolution))\n","\n","    # Predecir las etiquetas para cada punto de la malla\n","    Z = clf.predict(np.c_[xx1.ravel(), xx2.ravel()])\n","    Z = Z.reshape(xx1.shape)\n","\n","    # Crear la superficie de decisión\n","    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)\n","\n","    # Ajustar límites de los ejes\n","    plt.xlim(xx1.min(), xx1.max())\n","    plt.ylim(xx2.min(), xx2.max())\n","\n","    # Graficar los puntos de datos\n","    for idx, cl in enumerate(np.unique(y)):\n","        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1], alpha=0.8, color=colors[idx],\n","                    edgecolor='k', marker=markers[idx], label=f'Clase {cl}', s=100)\n","\n","# Graficar las regiones de decisión usando el modelo ya entrenado (xgb_model)\n","plot_decision_region(xgb_model, X_train_z, y_train)\n","\n","# Configurar el título y las etiquetas del gráfico\n","plt.title('XGBoost - Regiones de Decisión')\n","plt.xlabel('Característica 1 (Estandarizada)')\n","plt.ylabel('Característica 2 (Estandarizada)')\n","plt.legend(loc='upper left')\n","plt.grid(True)\n","plt.show()\n"],"metadata":{"id":"LcmhKajT5nzs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","#### **Principales parámetros de XGBoost**:\n","- **n_estimators**: Número de árboles.\n","- **learning_rate**: Tasa de aprendizaje para reducir el impacto de cada árbol.\n","- **max_depth**: Profundidad máxima de los árboles, controla el sobreajuste.\n","- **random_state**: Semilla para reproducibilidad.\n","\n","---\n","\n","### **LightGBM (Light Gradient Boosting Machine)**\n","\n","**LightGBM** es una implementación eficiente de **Gradient Boosting**, diseñada para ser rápida y escalable, especialmente para grandes volúmenes de datos. A diferencia de XGBoost, LightGBM crece los árboles de decisión de forma **hoja a hoja** (leaf-wise), lo que mejora la precisión sin aumentar significativamente el tiempo de cómputo.\n","\n","#### **Pasos para aplicar LightGBM**:\n","1. **Cargar y preparar los datos**.\n","2. **Dividir los datos en conjuntos de entrenamiento y prueba**.\n","3. **Entrenar el modelo LightGBM**.\n","4. **Evaluar el modelo**.\n"],"metadata":{"id":"gDmQyxF75AE_"}},{"cell_type":"code","source":["\n","import lightgbm as lgb\n","\n","from sklearn.metrics import accuracy_score\n","\n","\n","# Crear el modelo LightGBM\n","lgb_model = lgb.LGBMClassifier(n_estimators=30, learning_rate=0.1, max_depth=3, random_state=1)\n","\n","# Entrenar el modelo con los datos de entrenamiento\n","lgb_model.fit(X_train_z, y_train)\n","\n","# Hacer predicciones en el conjunto de prueba\n","y_pred = lgb_model.predict(X_test_z)\n","\n","# Evaluar el rendimiento del modelo\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f'Precisión del modelo LightGBM: {accuracy:.4f}')\n","warnings.filterwarnings('ignore')  # Ignorar todos los warnings"],"metadata":{"id":"64R7sXeD3IE9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Definir la función de visualización para las regiones de decisión (igual que antes)\n","def plot_decision_region(clf, X, y, resolution=0.02):\n","    # Crear una paleta de colores y marcar diferentes clases\n","    markers = ('o', 's', '^', 'v')\n","    colors = ('#FF9999', '#66B2FF', '#99FF99')\n","    cmap = ListedColormap(colors[:len(np.unique(y))])\n","\n","    # Seleccionar solo las primeras dos características para la visualización\n","    X = X.iloc[:, :2].values  # Convertir a numpy array con las dos primeras características\n","\n","    # Crear los límites de los ejes\n","    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n","    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n","    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n","                           np.arange(x2_min, x2_max, resolution))\n","\n","    # Predecir las etiquetas para cada punto de la malla\n","    Z = clf.predict(np.c_[xx1.ravel(), xx2.ravel()])\n","    Z = Z.reshape(xx1.shape)\n","\n","    # Crear la superficie de decisión\n","    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)\n","\n","    # Ajustar límites de los ejes\n","    plt.xlim(xx1.min(), xx1.max())\n","    plt.ylim(xx2.min(), xx2.max())\n","\n","    # Graficar los puntos de datos\n","    for idx, cl in enumerate(np.unique(y)):\n","        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1], alpha=0.8, color=colors[idx],\n","                    edgecolor='k', marker=markers[idx], label=f'Clase {cl}', s=100)\n","\n","# Graficar las regiones de decisión usando el modelo ya entrenado (lgb_model)\n","plot_decision_region(lgb_model, X_train_z, y_train)\n","\n","# Configurar el título y las etiquetas del gráfico\n","plt.title('LightGBM - Regiones de Decisión')\n","plt.xlabel('Característica 1 (Estandarizada)')\n","plt.ylabel('Característica 2 (Estandarizada)')\n","plt.legend(loc='upper left')\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"J3giwXYD53wq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Principales parámetros de LightGBM**:\n","- **n_estimators**: Número de árboles.\n","- **learning_rate**: Tasa de aprendizaje.\n","- **max_depth**: Profundidad máxima de los árboles.\n","- **boosting_type**: Controla el tipo de boosting, puede ser 'gbdt' (por defecto) o 'dart'.\n","- **random_state**: Semilla para reproducibilidad.\n","\n","---\n","\n","### **Comparación entre XGBoost y LightGBM**:\n","- **XGBoost** es generalmente más flexible y ofrece más control en el ajuste de hiperparámetros, pero puede ser más lento en comparación con **LightGBM**.\n","- **LightGBM** es extremadamente eficiente para grandes conjuntos de datos y puede ser más rápido debido a su enfoque de crecimiento de árbol basado en hojas (leaf-wise), pero tiende a sobreajustar si no se controla adecuadamente.\n","\n","Ambos algoritmos son muy poderosos y se pueden ajustar a problemas de clasificación y regresión. Puedes probar ambos en tu conjunto de datos y elegir el que te ofrezca mejor rendimiento en términos de precisión y velocidad."],"metadata":{"id":"0OqY_6Eu3_E2"}},{"cell_type":"markdown","source":["# Conclusiones\n","\n","En resumen, todos los clasificadores estudiados (SVM, Árboles de Decisión, Random Forest, XGBoost, y LightGBM) tienen características y fortalezas particulares que los hacen más adecuados para diferentes tipos de problemas. **SVM** es eficaz cuando se trata de problemas con fronteras de decisión claras y es particularmente útil en conjuntos de datos de alta dimensionalidad, aunque su rendimiento puede disminuir con grandes volúmenes de datos. Los **Árboles de Decisión** son fáciles de interpretar y visualmente intuitivos, pero tienden a sobreajustarse fácilmente si no se controlan adecuadamente.\n","\n","**Random Forest**, al combinar varios árboles de decisión, mejora la robustez y reduce el riesgo de sobreajuste, además de ser eficaz en problemas donde se requiere interpretabilidad y estabilidad. Por otro lado, **XGBoost** y **LightGBM** sobresalen en escenarios donde se necesita alta precisión y eficiencia en grandes conjuntos de datos, utilizando técnicas avanzadas de **boosting** para corregir los errores iterativamente. Si bien todos los modelos ofrecen buenos resultados, **XGBoost** y **LightGBM** son preferidos en problemas más complejos, mientras que **Random Forest** y **SVM** son útiles para implementaciones más sencillas o cuando las características del problema favorecen estos enfoques."],"metadata":{"id":"gtTaA2mR7jOi"}}]}